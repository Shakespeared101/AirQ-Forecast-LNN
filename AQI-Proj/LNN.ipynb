{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITj5aBa23Bwz",
        "outputId": "8363f9a8-215a-47f7-a487-c561f26c66a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ncps\n",
            "  Downloading ncps-1.0.1-py3-none-any.whl.metadata (702 bytes)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ncps) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ncps) (24.2)\n",
            "Downloading ncps-1.0.1-py3-none-any.whl (60 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ncps\n",
            "Successfully installed ncps-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ncps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgrEc2xp3Pbq",
        "outputId": "994e2c2c-c5f5-4274-ba3d-59246af2603a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ncps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w536FOXR3RvY",
        "outputId": "919da8d2-0fdf-4461-d013-9d3788f73754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ncps in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ncps) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ncps) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8UC64KYn3VUU",
        "outputId": "2f413505-d04a-4e6e-b7db-5c7acc2909fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.70.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.12.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.33)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.33)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.1,>=0.5.1 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.20.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.37.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.1.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.4 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "efc50efe06544fffbf94da0d612a7af9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ncps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf-dHoHt3XFn",
        "outputId": "e7e7f91c-531c-4ca7-8da1-2bad548140ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ncps in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ncps) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ncps) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from ncps.wirings import AutoNCP\n",
        "from ncps.tf import LTCCell\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('delhi_aqi.csv')  # Replace with the actual file name\n",
        "\n",
        "# Convert 'date' column to datetime and extract useful features\n",
        "df['Datetime'] = pd.to_datetime(df['date'])\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['date', 'Datetime'], axis=1, inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "features = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Hour', 'DayOfWeek', 'Day', 'Month']\n",
        "target = 'co'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into 70% and 30%\n",
        "split_index = int(0.7 * len(X))\n",
        "X_train_test, X_predict = X[:split_index], X[split_index:]\n",
        "y_train_test, y_predict = y[:split_index], y[split_index:]\n",
        "\n",
        "# Further split the 70% data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LNN input\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_predict = X_predict.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n",
        "\n",
        "# Build the LNN model\n",
        "wiring = AutoNCP(32, output_size=1)  # 32 hidden units, output size 1\n",
        "ltc_cell = LTCCell(wiring)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1, X_train.shape[2])),  # Input shape (time steps, features)\n",
        "    tf.keras.layers.RNN(ltc_cell, return_sequences=False),  # LTC layer\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test set (part of the 70%)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test MAE (70% data): {test_mae:.4f}')\n",
        "\n",
        "# Predict and evaluate on the remaining 30%\n",
        "y_pred = model.predict(X_predict)\n",
        "mae_co = np.mean(np.abs(y_predict - y_pred))\n",
        "print(f'MAE on 30% prediction data: {mae_co:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRAkvdu63tiG",
        "outputId": "2718f997-3239-4118-db39-1269603ae664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "263/263 [==============================] - 14s 16ms/step - loss: 16845968.0000 - mae: 2960.8865 - val_loss: 18555832.0000 - val_mae: 3050.3481\n",
            "Epoch 2/100\n",
            "263/263 [==============================] - 5s 19ms/step - loss: 16824926.0000 - mae: 2957.3455 - val_loss: 18527064.0000 - val_mae: 3045.6289\n",
            "Epoch 3/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 16786788.0000 - mae: 2950.8521 - val_loss: 18479322.0000 - val_mae: 3037.7815\n",
            "Epoch 4/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 16734531.0000 - mae: 2942.0129 - val_loss: 18418348.0000 - val_mae: 3027.7273\n",
            "Epoch 5/100\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 16667517.0000 - mae: 2930.5491 - val_loss: 18340604.0000 - val_mae: 3014.8623\n",
            "Epoch 6/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 16583101.0000 - mae: 2916.1726 - val_loss: 18243860.0000 - val_mae: 2998.7744\n",
            "Epoch 7/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 16479768.0000 - mae: 2898.4368 - val_loss: 18127630.0000 - val_mae: 2979.3323\n",
            "Epoch 8/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 16356953.0000 - mae: 2877.1616 - val_loss: 17990510.0000 - val_mae: 2956.2310\n",
            "Epoch 9/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 16214435.0000 - mae: 2852.2517 - val_loss: 17833614.0000 - val_mae: 2929.5737\n",
            "Epoch 10/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 16052206.0000 - mae: 2823.5234 - val_loss: 17655232.0000 - val_mae: 2898.9702\n",
            "Epoch 11/100\n",
            "263/263 [==============================] - 4s 13ms/step - loss: 15870140.0000 - mae: 2791.2456 - val_loss: 17457396.0000 - val_mae: 2864.6438\n",
            "Epoch 12/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 15668764.0000 - mae: 2754.8894 - val_loss: 17239422.0000 - val_mae: 2826.3425\n",
            "Epoch 13/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 15448857.0000 - mae: 2714.6367 - val_loss: 17002756.0000 - val_mae: 2784.1604\n",
            "Epoch 14/100\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 15210823.0000 - mae: 2670.6367 - val_loss: 16747518.0000 - val_mae: 2737.9805\n",
            "Epoch 15/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 14955773.0000 - mae: 2622.8865 - val_loss: 16476328.0000 - val_mae: 2688.2590\n",
            "Epoch 16/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 14685147.0000 - mae: 2571.3247 - val_loss: 16187820.0000 - val_mae: 2635.0569\n",
            "Epoch 17/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 14400615.0000 - mae: 2516.5181 - val_loss: 15885563.0000 - val_mae: 2579.1621\n",
            "Epoch 18/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 14102934.0000 - mae: 2460.2246 - val_loss: 15570652.0000 - val_mae: 2521.3997\n",
            "Epoch 19/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 13793584.0000 - mae: 2401.2874 - val_loss: 15244656.0000 - val_mae: 2462.5151\n",
            "Epoch 20/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 13474394.0000 - mae: 2342.2126 - val_loss: 14907530.0000 - val_mae: 2402.5125\n",
            "Epoch 21/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 13147541.0000 - mae: 2282.0898 - val_loss: 14565096.0000 - val_mae: 2342.9729\n",
            "Epoch 22/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 12814954.0000 - mae: 2223.5791 - val_loss: 14215499.0000 - val_mae: 2284.3196\n",
            "Epoch 23/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 12478107.0000 - mae: 2167.7507 - val_loss: 13862754.0000 - val_mae: 2228.3269\n",
            "Epoch 24/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 12139522.0000 - mae: 2115.4080 - val_loss: 13508595.0000 - val_mae: 2175.1677\n",
            "Epoch 25/100\n",
            "263/263 [==============================] - 5s 18ms/step - loss: 11801441.0000 - mae: 2065.5923 - val_loss: 13155767.0000 - val_mae: 2126.7385\n",
            "Epoch 26/100\n",
            "263/263 [==============================] - 5s 20ms/step - loss: 11466523.0000 - mae: 2021.1670 - val_loss: 12808124.0000 - val_mae: 2084.4858\n",
            "Epoch 27/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 11137102.0000 - mae: 1982.5469 - val_loss: 12463655.0000 - val_mae: 2048.2935\n",
            "Epoch 28/100\n",
            "263/263 [==============================] - 4s 13ms/step - loss: 10815773.0000 - mae: 1948.2375 - val_loss: 12130007.0000 - val_mae: 2017.9506\n",
            "Epoch 29/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 10505362.0000 - mae: 1920.5786 - val_loss: 11808001.0000 - val_mae: 1993.8351\n",
            "Epoch 30/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 10207607.0000 - mae: 1897.7516 - val_loss: 11499155.0000 - val_mae: 1975.9811\n",
            "Epoch 31/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 9924484.0000 - mae: 1881.5099 - val_loss: 11206746.0000 - val_mae: 1963.6210\n",
            "Epoch 32/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 9658120.0000 - mae: 1871.8701 - val_loss: 10929720.0000 - val_mae: 1957.1522\n",
            "Epoch 33/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 9410279.0000 - mae: 1866.5090 - val_loss: 10675153.0000 - val_mae: 1956.3817\n",
            "Epoch 34/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 9183949.0000 - mae: 1865.3624 - val_loss: 10438989.0000 - val_mae: 1960.5599\n",
            "Epoch 35/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8980524.0000 - mae: 1870.2881 - val_loss: 10229172.0000 - val_mae: 1969.1115\n",
            "Epoch 36/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 8800508.0000 - mae: 1878.9147 - val_loss: 10044964.0000 - val_mae: 1981.1340\n",
            "Epoch 37/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8643826.0000 - mae: 1890.7784 - val_loss: 9881100.0000 - val_mae: 1996.5138\n",
            "Epoch 38/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 8511030.0000 - mae: 1905.6246 - val_loss: 9744105.0000 - val_mae: 2014.7880\n",
            "Epoch 39/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 8401055.0000 - mae: 1922.9135 - val_loss: 9628942.0000 - val_mae: 2035.2107\n",
            "Epoch 40/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8312270.0000 - mae: 1941.4640 - val_loss: 9534694.0000 - val_mae: 2056.3772\n",
            "Epoch 41/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8243145.5000 - mae: 1962.1324 - val_loss: 9461142.0000 - val_mae: 2076.9563\n",
            "Epoch 42/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 8191102.0000 - mae: 1980.6849 - val_loss: 9404080.0000 - val_mae: 2096.9685\n",
            "Epoch 43/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 8152807.0000 - mae: 1997.7549 - val_loss: 9362317.0000 - val_mae: 2115.0159\n",
            "Epoch 44/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 8125779.5000 - mae: 2014.5780 - val_loss: 9330028.0000 - val_mae: 2132.3176\n",
            "Epoch 45/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8107917.5000 - mae: 2029.4692 - val_loss: 9308514.0000 - val_mae: 2146.4094\n",
            "Epoch 46/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 6849429.5000 - mae: 1455.4001 - val_loss: 7262205.5000 - val_mae: 1235.2527\n",
            "Epoch 47/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 5861986.0000 - mae: 1089.7620 - val_loss: 6761019.5000 - val_mae: 1176.5431\n",
            "Epoch 48/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 5440824.0000 - mae: 1023.1172 - val_loss: 6320619.5000 - val_mae: 1094.1509\n",
            "Epoch 49/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 5073603.5000 - mae: 965.2009 - val_loss: 5934404.5000 - val_mae: 1044.1818\n",
            "Epoch 50/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4741318.5000 - mae: 916.5145 - val_loss: 5570426.0000 - val_mae: 994.0501\n",
            "Epoch 51/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4432548.0000 - mae: 868.6180 - val_loss: 5236061.0000 - val_mae: 951.1735\n",
            "Epoch 52/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4145310.2500 - mae: 823.7474 - val_loss: 4919872.5000 - val_mae: 899.7133\n",
            "Epoch 53/100\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 3874448.2500 - mae: 778.9372 - val_loss: 4620284.5000 - val_mae: 852.4661\n",
            "Epoch 54/100\n",
            "263/263 [==============================] - 4s 13ms/step - loss: 3618251.5000 - mae: 735.3795 - val_loss: 4335659.0000 - val_mae: 821.2891\n",
            "Epoch 55/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 3372584.0000 - mae: 685.4813 - val_loss: 4053540.0000 - val_mae: 751.9090\n",
            "Epoch 56/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 3138333.7500 - mae: 630.4745 - val_loss: 3790139.0000 - val_mae: 694.8352\n",
            "Epoch 57/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2918288.5000 - mae: 582.8893 - val_loss: 3542047.7500 - val_mae: 660.5160\n",
            "Epoch 58/100\n",
            "263/263 [==============================] - 5s 18ms/step - loss: 2714114.7500 - mae: 550.3639 - val_loss: 3310038.0000 - val_mae: 623.5178\n",
            "Epoch 59/100\n",
            "263/263 [==============================] - 4s 15ms/step - loss: 2521829.7500 - mae: 520.7068 - val_loss: 3089283.7500 - val_mae: 594.1163\n",
            "Epoch 60/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 2340241.7500 - mae: 492.6201 - val_loss: 2878366.7500 - val_mae: 560.2880\n",
            "Epoch 61/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 2169384.2500 - mae: 465.6906 - val_loss: 2680901.5000 - val_mae: 539.7950\n",
            "Epoch 62/100\n",
            "263/263 [==============================] - 4s 14ms/step - loss: 2008753.2500 - mae: 439.5644 - val_loss: 2492307.2500 - val_mae: 502.8428\n",
            "Epoch 63/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1858073.7500 - mae: 414.7731 - val_loss: 2316310.5000 - val_mae: 484.6563\n",
            "Epoch 64/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1716142.1250 - mae: 394.3944 - val_loss: 2148279.5000 - val_mae: 456.4789\n",
            "Epoch 65/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1582568.3750 - mae: 371.5050 - val_loss: 1990168.6250 - val_mae: 443.2345\n",
            "Epoch 66/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1457563.0000 - mae: 349.7772 - val_loss: 1840420.2500 - val_mae: 410.0487\n",
            "Epoch 67/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 1340747.7500 - mae: 330.6589 - val_loss: 1699400.5000 - val_mae: 391.2580\n",
            "Epoch 68/100\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 1232428.8750 - mae: 314.2590 - val_loss: 1569907.0000 - val_mae: 379.8741\n",
            "Epoch 69/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 1131420.2500 - mae: 296.5516 - val_loss: 1443700.6250 - val_mae: 345.6664\n",
            "Epoch 70/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1036839.3750 - mae: 277.3023 - val_loss: 1333470.2500 - val_mae: 359.7668\n",
            "Epoch 71/100\n",
            "263/263 [==============================] - 3s 13ms/step - loss: 949711.0625 - mae: 262.6733 - val_loss: 1219213.8750 - val_mae: 309.7840\n",
            "Epoch 72/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 869475.1250 - mae: 251.4650 - val_loss: 1124883.3750 - val_mae: 320.0548\n",
            "Epoch 73/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 794695.0625 - mae: 237.5405 - val_loss: 1027380.2500 - val_mae: 275.7835\n",
            "Epoch 74/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 725781.2500 - mae: 226.0742 - val_loss: 942052.8750 - val_mae: 265.7389\n",
            "Epoch 75/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 661785.7500 - mae: 212.7971 - val_loss: 862262.8125 - val_mae: 257.3503\n",
            "Epoch 76/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 603824.3125 - mae: 206.6357 - val_loss: 789155.5000 - val_mae: 238.4393\n",
            "Epoch 77/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 549158.3125 - mae: 194.0670 - val_loss: 720185.0000 - val_mae: 221.4238\n",
            "Epoch 78/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 499221.1875 - mae: 184.6133 - val_loss: 659215.7500 - val_mae: 214.6421\n",
            "Epoch 79/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 453684.8750 - mae: 175.5775 - val_loss: 603500.7500 - val_mae: 220.4624\n",
            "Epoch 80/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 411931.6250 - mae: 167.8288 - val_loss: 550778.6250 - val_mae: 203.0270\n",
            "Epoch 81/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 373259.4688 - mae: 157.8291 - val_loss: 499255.0938 - val_mae: 180.2063\n",
            "Epoch 82/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 339118.5000 - mae: 151.5166 - val_loss: 455762.5312 - val_mae: 167.9569\n",
            "Epoch 83/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 307513.7812 - mae: 142.7560 - val_loss: 415709.1562 - val_mae: 160.6500\n",
            "Epoch 84/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 279769.2188 - mae: 139.1044 - val_loss: 379740.2188 - val_mae: 159.2657\n",
            "Epoch 85/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 253597.2031 - mae: 131.1285 - val_loss: 348246.0625 - val_mae: 158.1104\n",
            "Epoch 86/100\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 231098.5625 - mae: 128.6551 - val_loss: 316325.6250 - val_mae: 144.9499\n",
            "Epoch 87/100\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 210869.5156 - mae: 127.4057 - val_loss: 290490.8125 - val_mae: 139.4287\n",
            "Epoch 88/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 190598.3438 - mae: 116.5565 - val_loss: 265753.4688 - val_mae: 137.3793\n",
            "Epoch 89/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 173723.4688 - mae: 113.2883 - val_loss: 243467.3438 - val_mae: 133.0288\n",
            "Epoch 90/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 159037.9219 - mae: 112.5118 - val_loss: 221452.5000 - val_mae: 127.6809\n",
            "Epoch 91/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 144188.6875 - mae: 104.8081 - val_loss: 200737.8125 - val_mae: 113.9595\n",
            "Epoch 92/100\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 130909.3750 - mae: 98.1711 - val_loss: 182893.2812 - val_mae: 113.5296\n",
            "Epoch 93/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 119701.6328 - mae: 96.7917 - val_loss: 168500.6562 - val_mae: 115.7329\n",
            "Epoch 94/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 109014.2812 - mae: 91.4626 - val_loss: 151844.5625 - val_mae: 99.4499\n",
            "Epoch 95/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 99573.0469 - mae: 88.4664 - val_loss: 138337.0469 - val_mae: 95.3607\n",
            "Epoch 96/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 90444.4219 - mae: 82.4989 - val_loss: 130019.2734 - val_mae: 107.6729\n",
            "Epoch 97/100\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 82427.3438 - mae: 77.5508 - val_loss: 113886.3828 - val_mae: 87.1019\n",
            "Epoch 98/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 75536.1562 - mae: 74.2812 - val_loss: 106523.7500 - val_mae: 92.4184\n",
            "Epoch 99/100\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 68963.7344 - mae: 69.6286 - val_loss: 94264.1562 - val_mae: 73.2972\n",
            "Epoch 100/100\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 62867.2227 - mae: 64.1563 - val_loss: 87746.5547 - val_mae: 79.7519\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 54743.7461 - mae: 69.1593\n",
            "Test MAE (70% data): 69.1593\n",
            "177/177 [==============================] - 1s 3ms/step\n",
            "MAE on 30% prediction data: 2578.1093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from ncps.wirings import AutoNCP\n",
        "from ncps.tf import LTCCell\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('delhi_aqi.csv')  # Replace with the actual file name\n",
        "\n",
        "# Convert 'date' column to datetime and extract useful features\n",
        "df['Datetime'] = pd.to_datetime(df['date'])\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['date', 'Datetime'], axis=1, inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "features = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Hour', 'DayOfWeek', 'Day', 'Month']\n",
        "target = 'no'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into 70% and 30%\n",
        "split_index = int(0.7 * len(X))\n",
        "X_train_test, X_predict = X[:split_index], X[split_index:]\n",
        "y_train_test, y_predict = y[:split_index], y[split_index:]\n",
        "\n",
        "# Further split the 70% data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LNN input\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_predict = X_predict.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n",
        "\n",
        "# Build the LNN model\n",
        "wiring = AutoNCP(32, output_size=1)  # 32 hidden units, output size 1\n",
        "ltc_cell = LTCCell(wiring)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1, X_train.shape[2])),  # Input shape (time steps, features)\n",
        "    tf.keras.layers.RNN(ltc_cell, return_sequences=False),  # LTC layer\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test set (part of the 70%)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test MAE (70% data): {test_mae:.4f}')\n",
        "\n",
        "# Predict and evaluate on the remaining 30%\n",
        "y_pred = model.predict(X_predict)\n",
        "mae_no = np.mean(np.abs(y_predict - y_pred))\n",
        "print(f'MAE on 30% prediction data: {mae_no:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm3-DwR_4GXX",
        "outputId": "5316717a-b0c6-4abb-df3f-1ab9bb41809e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "263/263 [==============================] - 13s 12ms/step - loss: 4793.6875 - mae: 32.7361 - val_loss: 5388.3979 - val_mae: 34.6432\n",
            "Epoch 2/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4606.8140 - mae: 32.2198 - val_loss: 5165.7114 - val_mae: 34.5524\n",
            "Epoch 3/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4374.6162 - mae: 32.5539 - val_loss: 4893.4019 - val_mae: 35.2708\n",
            "Epoch 4/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4134.2622 - mae: 33.6885 - val_loss: 4639.1338 - val_mae: 36.8253\n",
            "Epoch 5/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 3916.0696 - mae: 35.5459 - val_loss: 4389.6807 - val_mae: 38.6498\n",
            "Epoch 6/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3555.4705 - mae: 32.2563 - val_loss: 3658.2778 - val_mae: 23.7976\n",
            "Epoch 7/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2844.3450 - mae: 20.0264 - val_loss: 3061.2251 - val_mae: 20.6429\n",
            "Epoch 8/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2352.8840 - mae: 17.3724 - val_loss: 2549.1987 - val_mae: 18.3346\n",
            "Epoch 9/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1934.0800 - mae: 15.1311 - val_loss: 2106.1382 - val_mae: 15.8565\n",
            "Epoch 10/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 1579.7771 - mae: 13.1356 - val_loss: 1733.8958 - val_mae: 13.8735\n",
            "Epoch 11/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1283.6973 - mae: 11.5567 - val_loss: 1421.0557 - val_mae: 11.9036\n",
            "Epoch 12/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1038.4803 - mae: 10.0688 - val_loss: 1163.6748 - val_mae: 10.5775\n",
            "Epoch 13/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 835.0079 - mae: 8.6448 - val_loss: 947.3207 - val_mae: 8.8793\n",
            "Epoch 14/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 664.2006 - mae: 7.0157 - val_loss: 766.0406 - val_mae: 7.2953\n",
            "Epoch 15/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 526.2840 - mae: 5.7530 - val_loss: 618.4348 - val_mae: 6.0004\n",
            "Epoch 16/50\n",
            "263/263 [==============================] - 6s 23ms/step - loss: 417.3851 - mae: 4.9932 - val_loss: 502.9377 - val_mae: 5.2445\n",
            "Epoch 17/50\n",
            "263/263 [==============================] - 5s 19ms/step - loss: 330.8723 - mae: 4.4605 - val_loss: 409.8030 - val_mae: 5.0308\n",
            "Epoch 18/50\n",
            "263/263 [==============================] - 4s 16ms/step - loss: 262.6047 - mae: 4.0010 - val_loss: 334.6088 - val_mae: 4.1828\n",
            "Epoch 19/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 209.7816 - mae: 3.6624 - val_loss: 275.9247 - val_mae: 3.9263\n",
            "Epoch 20/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 168.1310 - mae: 3.2810 - val_loss: 230.8772 - val_mae: 3.7810\n",
            "Epoch 21/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 136.7380 - mae: 3.1122 - val_loss: 196.6788 - val_mae: 3.8266\n",
            "Epoch 22/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 111.3161 - mae: 2.8329 - val_loss: 165.3219 - val_mae: 3.0355\n",
            "Epoch 23/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 91.5896 - mae: 2.6251 - val_loss: 140.2252 - val_mae: 2.7075\n",
            "Epoch 24/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 75.8322 - mae: 2.4450 - val_loss: 120.5938 - val_mae: 2.5503\n",
            "Epoch 25/50\n",
            "263/263 [==============================] - 4s 14ms/step - loss: 63.5727 - mae: 2.3567 - val_loss: 108.8110 - val_mae: 3.2908\n",
            "Epoch 26/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 53.5979 - mae: 2.2549 - val_loss: 92.5216 - val_mae: 2.3598\n",
            "Epoch 27/50\n",
            "263/263 [==============================] - 3s 13ms/step - loss: 44.9444 - mae: 2.1032 - val_loss: 80.5432 - val_mae: 2.2532\n",
            "Epoch 28/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 38.2277 - mae: 2.0199 - val_loss: 71.1516 - val_mae: 2.2218\n",
            "Epoch 29/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 32.1620 - mae: 1.8710 - val_loss: 64.8422 - val_mae: 2.1630\n",
            "Epoch 30/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 28.0009 - mae: 1.8686 - val_loss: 56.6737 - val_mae: 2.2068\n",
            "Epoch 31/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 24.2103 - mae: 1.8507 - val_loss: 49.7464 - val_mae: 1.8903\n",
            "Epoch 32/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 20.9438 - mae: 1.7728 - val_loss: 45.1880 - val_mae: 1.9474\n",
            "Epoch 33/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 17.6169 - mae: 1.5976 - val_loss: 39.4623 - val_mae: 1.5451\n",
            "Epoch 34/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 15.2581 - mae: 1.5185 - val_loss: 35.3786 - val_mae: 1.4940\n",
            "Epoch 35/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 13.3892 - mae: 1.5072 - val_loss: 31.8632 - val_mae: 1.4782\n",
            "Epoch 36/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 11.9959 - mae: 1.5219 - val_loss: 29.1768 - val_mae: 1.5964\n",
            "Epoch 37/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 10.7024 - mae: 1.5067 - val_loss: 29.1156 - val_mae: 1.8184\n",
            "Epoch 38/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 9.5272 - mae: 1.4356 - val_loss: 23.5679 - val_mae: 1.2868\n",
            "Epoch 39/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8.2608 - mae: 1.3714 - val_loss: 22.2954 - val_mae: 1.3468\n",
            "Epoch 40/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 7.3678 - mae: 1.3231 - val_loss: 20.1594 - val_mae: 1.3757\n",
            "Epoch 41/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 7.7023 - mae: 1.5137 - val_loss: 18.4458 - val_mae: 1.3870\n",
            "Epoch 42/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 6.0571 - mae: 1.2411 - val_loss: 17.1996 - val_mae: 1.5974\n",
            "Epoch 43/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 6.1227 - mae: 1.3876 - val_loss: 15.3240 - val_mae: 1.2768\n",
            "Epoch 44/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4.8612 - mae: 1.1784 - val_loss: 14.4774 - val_mae: 1.4695\n",
            "Epoch 45/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4.7939 - mae: 1.2224 - val_loss: 14.4159 - val_mae: 1.7430\n",
            "Epoch 46/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4.3932 - mae: 1.1646 - val_loss: 11.5801 - val_mae: 1.0345\n",
            "Epoch 47/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 4.0454 - mae: 1.1594 - val_loss: 10.7976 - val_mae: 1.0646\n",
            "Epoch 48/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4.1117 - mae: 1.2294 - val_loss: 16.3147 - val_mae: 2.5776\n",
            "Epoch 49/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3.7787 - mae: 1.1729 - val_loss: 10.6265 - val_mae: 1.4540\n",
            "Epoch 50/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3.9521 - mae: 1.2265 - val_loss: 8.5875 - val_mae: 0.9493\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 4.0395 - mae: 0.8698\n",
            "Test MAE (70% data): 0.8698\n",
            "177/177 [==============================] - 1s 3ms/step\n",
            "MAE on 30% prediction data: 49.6705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from ncps.wirings import AutoNCP\n",
        "from ncps.tf import LTCCell\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('delhi_aqi.csv')  # Replace with the actual file name\n",
        "\n",
        "# Convert 'date' column to datetime and extract useful features\n",
        "df['Datetime'] = pd.to_datetime(df['date'])\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['date', 'Datetime'], axis=1, inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "features = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Hour', 'DayOfWeek', 'Day', 'Month']\n",
        "target = 'no2'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into 70% and 30%\n",
        "split_index = int(0.7 * len(X))\n",
        "X_train_test, X_predict = X[:split_index], X[split_index:]\n",
        "y_train_test, y_predict = y[:split_index], y[split_index:]\n",
        "\n",
        "# Further split the 70% data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LNN input\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_predict = X_predict.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n",
        "\n",
        "# Build the LNN model\n",
        "wiring = AutoNCP(32, output_size=1)  # 32 hidden units, output size 1\n",
        "ltc_cell = LTCCell(wiring)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1, X_train.shape[2])),  # Input shape (time steps, features)\n",
        "    tf.keras.layers.RNN(ltc_cell, return_sequences=False),  # LTC layer\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test set (part of the 70%)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test MAE (70% data): {test_mae:.4f}')\n",
        "\n",
        "# Predict and evaluate on the remaining 30%\n",
        "y_pred = model.predict(X_predict)\n",
        "mae_no2 = np.mean(np.abs(y_predict - y_pred))\n",
        "print(f'MAE on 30% prediction data: {mae_no2:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1TdNbjy4O8H",
        "outputId": "f012bf5f-b0d2-41fa-bc55-016f94cbc090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "263/263 [==============================] - 12s 10ms/step - loss: 6449.2915 - mae: 64.5687 - val_loss: 6805.3193 - val_mae: 64.5998\n",
            "Epoch 2/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 6007.0322 - mae: 61.0341 - val_loss: 6293.0059 - val_mae: 60.5146\n",
            "Epoch 3/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 5404.0327 - mae: 55.9780 - val_loss: 5566.6753 - val_mae: 54.4931\n",
            "Epoch 4/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4592.6064 - mae: 49.0064 - val_loss: 4669.6050 - val_mae: 46.8484\n",
            "Epoch 5/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3778.5151 - mae: 41.9864 - val_loss: 3877.4402 - val_mae: 40.2852\n",
            "Epoch 6/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3083.6768 - mae: 36.5939 - val_loss: 3213.8547 - val_mae: 36.0083\n",
            "Epoch 7/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2591.7937 - mae: 33.7619 - val_loss: 2842.3867 - val_mae: 34.7447\n",
            "Epoch 8/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 2370.8665 - mae: 33.4093 - val_loss: 2696.6172 - val_mae: 35.0407\n",
            "Epoch 9/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2288.8784 - mae: 33.7829 - val_loss: 2572.8782 - val_mae: 34.3040\n",
            "Epoch 10/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1913.2152 - mae: 28.5860 - val_loss: 1831.8580 - val_mae: 21.4635\n",
            "Epoch 11/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1240.6628 - mae: 16.3886 - val_loss: 1339.5398 - val_mae: 14.5873\n",
            "Epoch 12/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 896.5740 - mae: 11.7553 - val_loss: 1046.4868 - val_mae: 13.1286\n",
            "Epoch 13/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 679.6813 - mae: 9.5921 - val_loss: 819.3734 - val_mae: 9.6817\n",
            "Epoch 14/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 523.3345 - mae: 7.6308 - val_loss: 660.4464 - val_mae: 8.6349\n",
            "Epoch 15/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 408.2121 - mae: 6.1695 - val_loss: 530.6443 - val_mae: 6.6222\n",
            "Epoch 16/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 321.5217 - mae: 5.0526 - val_loss: 432.8857 - val_mae: 5.4549\n",
            "Epoch 17/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 257.3608 - mae: 4.4486 - val_loss: 357.6292 - val_mae: 4.7246\n",
            "Epoch 18/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 207.3702 - mae: 3.8074 - val_loss: 297.3643 - val_mae: 4.1738\n",
            "Epoch 19/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 168.5928 - mae: 3.3011 - val_loss: 251.2512 - val_mae: 4.0011\n",
            "Epoch 20/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 138.6324 - mae: 3.0179 - val_loss: 211.9702 - val_mae: 3.7391\n",
            "Epoch 21/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 114.5056 - mae: 2.6920 - val_loss: 179.7483 - val_mae: 3.2533\n",
            "Epoch 22/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 95.4971 - mae: 2.4578 - val_loss: 154.3940 - val_mae: 2.9737\n",
            "Epoch 23/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 80.4010 - mae: 2.2764 - val_loss: 133.0960 - val_mae: 2.5547\n",
            "Epoch 24/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 68.5785 - mae: 2.1980 - val_loss: 118.6968 - val_mae: 2.8013\n",
            "Epoch 25/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 57.9707 - mae: 1.9255 - val_loss: 102.0112 - val_mae: 2.4376\n",
            "Epoch 26/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 49.8436 - mae: 1.8270 - val_loss: 88.9503 - val_mae: 1.9413\n",
            "Epoch 27/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 42.5078 - mae: 1.6215 - val_loss: 80.4767 - val_mae: 2.3256\n",
            "Epoch 28/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 36.5268 - mae: 1.4601 - val_loss: 69.4035 - val_mae: 1.5944\n",
            "Epoch 29/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 31.1500 - mae: 1.2314 - val_loss: 61.0843 - val_mae: 1.2688\n",
            "Epoch 30/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 26.7866 - mae: 1.0587 - val_loss: 54.7468 - val_mae: 1.2702\n",
            "Epoch 31/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 23.4033 - mae: 1.0329 - val_loss: 51.3889 - val_mae: 1.5901\n",
            "Epoch 32/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 20.3858 - mae: 0.9324 - val_loss: 44.5177 - val_mae: 0.9024\n",
            "Epoch 33/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 18.1070 - mae: 0.9266 - val_loss: 40.8455 - val_mae: 1.0959\n",
            "Epoch 34/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 15.8574 - mae: 0.8408 - val_loss: 36.7791 - val_mae: 0.9915\n",
            "Epoch 35/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 13.9741 - mae: 0.7695 - val_loss: 34.5183 - val_mae: 1.3207\n",
            "Epoch 36/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 12.3672 - mae: 0.7131 - val_loss: 30.4648 - val_mae: 0.7285\n",
            "Epoch 37/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 10.9124 - mae: 0.6606 - val_loss: 28.3089 - val_mae: 1.0873\n",
            "Epoch 38/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 10.0233 - mae: 0.7417 - val_loss: 25.5414 - val_mae: 0.6340\n",
            "Epoch 39/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8.7242 - mae: 0.5977 - val_loss: 23.2630 - val_mae: 0.6129\n",
            "Epoch 40/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 7.9769 - mae: 0.6460 - val_loss: 21.6426 - val_mae: 0.8011\n",
            "Epoch 41/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 7.1994 - mae: 0.6300 - val_loss: 19.6504 - val_mae: 0.5854\n",
            "Epoch 42/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 6.7657 - mae: 0.7174 - val_loss: 20.0297 - val_mae: 1.1428\n",
            "Epoch 43/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 5.9767 - mae: 0.6496 - val_loss: 16.7442 - val_mae: 0.6059\n",
            "Epoch 44/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 5.4353 - mae: 0.6785 - val_loss: 15.3104 - val_mae: 0.6131\n",
            "Epoch 45/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 4.8639 - mae: 0.6427 - val_loss: 14.2271 - val_mae: 0.6191\n",
            "Epoch 46/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 4.3368 - mae: 0.5937 - val_loss: 13.1556 - val_mae: 0.6769\n",
            "Epoch 47/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3.9712 - mae: 0.6243 - val_loss: 12.8447 - val_mae: 0.8954\n",
            "Epoch 48/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3.8517 - mae: 0.6936 - val_loss: 11.6435 - val_mae: 0.8246\n",
            "Epoch 49/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3.3234 - mae: 0.6220 - val_loss: 10.0354 - val_mae: 0.6009\n",
            "Epoch 50/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 3.0467 - mae: 0.6656 - val_loss: 8.9881 - val_mae: 0.4716\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 1.1993 - mae: 0.3870\n",
            "Test MAE (70% data): 0.3870\n",
            "177/177 [==============================] - 1s 3ms/step\n",
            "MAE on 30% prediction data: 47.9853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from ncps.wirings import AutoNCP\n",
        "from ncps.tf import LTCCell\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('delhi_aqi.csv')  # Replace with the actual file name\n",
        "\n",
        "# Convert 'date' column to datetime and extract useful features\n",
        "df['Datetime'] = pd.to_datetime(df['date'])\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['date', 'Datetime'], axis=1, inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "features = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Hour', 'DayOfWeek', 'Day', 'Month']\n",
        "target = 'o3'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into 70% and 30%\n",
        "split_index = int(0.7 * len(X))\n",
        "X_train_test, X_predict = X[:split_index], X[split_index:]\n",
        "y_train_test, y_predict = y[:split_index], y[split_index:]\n",
        "\n",
        "# Further split the 70% data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LNN input\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_predict = X_predict.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n",
        "\n",
        "# Build the LNN model\n",
        "wiring = AutoNCP(32, output_size=1)  # 32 hidden units, output size 1\n",
        "ltc_cell = LTCCell(wiring)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1, X_train.shape[2])),  # Input shape (time steps, features)\n",
        "    tf.keras.layers.RNN(ltc_cell, return_sequences=False),  # LTC layer\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test set (part of the 70%)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test MAE (70% data): {test_mae:.4f}')\n",
        "\n",
        "# Predict and evaluate on the remaining 30%\n",
        "y_pred = model.predict(X_predict)\n",
        "mae_o3 = np.mean(np.abs(y_predict - y_pred))\n",
        "print(f'MAE on 30% prediction data: {mae_o3:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56on7hhX4W8I",
        "outputId": "57c2e946-cfb9-49a3-c03e-f69ff46eebb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "263/263 [==============================] - 18s 16ms/step - loss: 11484.6387 - mae: 64.8710 - val_loss: 11795.0869 - val_mae: 66.1963\n",
            "Epoch 2/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 11234.5244 - mae: 64.0203 - val_loss: 11454.2725 - val_mae: 65.2105\n",
            "Epoch 3/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 10779.2500 - mae: 62.7793 - val_loss: 10798.4492 - val_mae: 63.6817\n",
            "Epoch 4/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 10024.1406 - mae: 61.1916 - val_loss: 9986.8594 - val_mae: 62.2082\n",
            "Epoch 5/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 9226.7793 - mae: 60.0593 - val_loss: 9168.0225 - val_mae: 61.2831\n",
            "Epoch 6/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 8493.4316 - mae: 59.6248 - val_loss: 8470.1426 - val_mae: 61.2785\n",
            "Epoch 7/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 7734.4741 - mae: 57.2009 - val_loss: 7508.8369 - val_mae: 55.6557\n",
            "Epoch 8/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 6520.9546 - mae: 47.5259 - val_loss: 5902.8604 - val_mae: 38.6566\n",
            "Epoch 9/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 5113.3916 - mae: 33.3070 - val_loss: 4733.0811 - val_mae: 32.1109\n",
            "Epoch 10/50\n",
            "263/263 [==============================] - 4s 16ms/step - loss: 4114.2852 - mae: 27.3639 - val_loss: 3762.9189 - val_mae: 25.8994\n",
            "Epoch 11/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 3289.9800 - mae: 22.0555 - val_loss: 2978.6331 - val_mae: 20.9818\n",
            "Epoch 12/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2634.0974 - mae: 17.9958 - val_loss: 2361.3599 - val_mae: 17.3176\n",
            "Epoch 13/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 2120.0898 - mae: 15.0234 - val_loss: 1884.6447 - val_mae: 14.9615\n",
            "Epoch 14/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 1720.7321 - mae: 12.5963 - val_loss: 1505.2847 - val_mae: 12.0086\n",
            "Epoch 15/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 1412.2446 - mae: 10.7529 - val_loss: 1228.0769 - val_mae: 11.0723\n",
            "Epoch 16/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1170.1289 - mae: 9.2670 - val_loss: 998.2179 - val_mae: 8.6753\n",
            "Epoch 17/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 978.6385 - mae: 8.0314 - val_loss: 823.6203 - val_mae: 7.6288\n",
            "Epoch 18/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 826.9250 - mae: 7.0988 - val_loss: 683.7957 - val_mae: 6.8558\n",
            "Epoch 19/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 703.8316 - mae: 6.3096 - val_loss: 571.6826 - val_mae: 6.0107\n",
            "Epoch 20/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 603.2432 - mae: 5.6214 - val_loss: 480.0021 - val_mae: 5.4901\n",
            "Epoch 21/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 518.8184 - mae: 4.9603 - val_loss: 405.9532 - val_mae: 5.2211\n",
            "Epoch 22/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 448.9360 - mae: 4.5032 - val_loss: 337.5129 - val_mae: 4.1872\n",
            "Epoch 23/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 389.9099 - mae: 4.0471 - val_loss: 282.0209 - val_mae: 3.5896\n",
            "Epoch 24/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 339.0063 - mae: 3.5177 - val_loss: 238.0935 - val_mae: 3.2342\n",
            "Epoch 25/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 295.5336 - mae: 3.1471 - val_loss: 201.7780 - val_mae: 3.3596\n",
            "Epoch 26/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 258.5192 - mae: 2.7826 - val_loss: 167.2223 - val_mae: 2.6197\n",
            "Epoch 27/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 227.3138 - mae: 2.5330 - val_loss: 140.1284 - val_mae: 2.2031\n",
            "Epoch 28/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 200.4344 - mae: 2.2522 - val_loss: 117.2829 - val_mae: 1.9762\n",
            "Epoch 29/50\n",
            "263/263 [==============================] - 3s 13ms/step - loss: 178.0551 - mae: 2.1619 - val_loss: 102.0021 - val_mae: 2.5769\n",
            "Epoch 30/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 158.5790 - mae: 2.0502 - val_loss: 85.8494 - val_mae: 2.2424\n",
            "Epoch 31/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 141.6832 - mae: 1.8784 - val_loss: 70.9269 - val_mae: 1.5722\n",
            "Epoch 32/50\n",
            "263/263 [==============================] - 4s 14ms/step - loss: 127.8088 - mae: 1.8504 - val_loss: 62.7113 - val_mae: 2.2719\n",
            "Epoch 33/50\n",
            "263/263 [==============================] - 5s 17ms/step - loss: 115.7589 - mae: 1.7681 - val_loss: 51.1972 - val_mae: 1.4790\n",
            "Epoch 34/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 105.3829 - mae: 1.6997 - val_loss: 44.6889 - val_mae: 1.6552\n",
            "Epoch 35/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 96.5855 - mae: 1.7010 - val_loss: 37.8260 - val_mae: 1.3639\n",
            "Epoch 36/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 88.1954 - mae: 1.5627 - val_loss: 33.0831 - val_mae: 1.2465\n",
            "Epoch 37/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 81.9963 - mae: 1.6946 - val_loss: 28.4170 - val_mae: 1.2654\n",
            "Epoch 38/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 74.8446 - mae: 1.5298 - val_loss: 25.2320 - val_mae: 1.3882\n",
            "Epoch 39/50\n",
            "263/263 [==============================] - 4s 15ms/step - loss: 68.9157 - mae: 1.4440 - val_loss: 21.1562 - val_mae: 1.1679\n",
            "Epoch 40/50\n",
            "263/263 [==============================] - 3s 13ms/step - loss: 63.4371 - mae: 1.3738 - val_loss: 18.1489 - val_mae: 1.0987\n",
            "Epoch 41/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 58.9813 - mae: 1.4011 - val_loss: 18.0137 - val_mae: 1.5189\n",
            "Epoch 42/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 54.9166 - mae: 1.4161 - val_loss: 13.9543 - val_mae: 1.2103\n",
            "Epoch 43/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 50.4881 - mae: 1.2729 - val_loss: 12.8607 - val_mae: 1.5489\n",
            "Epoch 44/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 47.7683 - mae: 1.4255 - val_loss: 9.9368 - val_mae: 1.0944\n",
            "Epoch 45/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 44.2044 - mae: 1.3095 - val_loss: 8.3435 - val_mae: 0.9901\n",
            "Epoch 46/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 41.0747 - mae: 1.2618 - val_loss: 7.2648 - val_mae: 1.0477\n",
            "Epoch 47/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 38.5815 - mae: 1.2491 - val_loss: 8.2095 - val_mae: 1.2981\n",
            "Epoch 48/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 36.6930 - mae: 1.3404 - val_loss: 5.1301 - val_mae: 0.9202\n",
            "Epoch 49/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 33.8494 - mae: 1.1830 - val_loss: 4.8539 - val_mae: 1.0609\n",
            "Epoch 50/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 31.5409 - mae: 1.0788 - val_loss: 3.2551 - val_mae: 0.7794\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 29.9194 - mae: 0.8437\n",
            "Test MAE (70% data): 0.8437\n",
            "177/177 [==============================] - 1s 3ms/step\n",
            "MAE on 30% prediction data: 65.6022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from ncps.wirings import AutoNCP\n",
        "from ncps.tf import LTCCell\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('delhi_aqi.csv')  # Replace with the actual file name\n",
        "\n",
        "# Convert 'date' column to datetime and extract useful features\n",
        "df['Datetime'] = pd.to_datetime(df['date'])\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['date', 'Datetime'], axis=1, inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "features = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Hour', 'DayOfWeek', 'Day', 'Month']\n",
        "target = 'so2'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into 70% and 30%\n",
        "split_index = int(0.7 * len(X))\n",
        "X_train_test, X_predict = X[:split_index], X[split_index:]\n",
        "y_train_test, y_predict = y[:split_index], y[split_index:]\n",
        "\n",
        "# Further split the 70% data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LNN input\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_predict = X_predict.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n",
        "\n",
        "# Build the LNN model\n",
        "wiring = AutoNCP(32, output_size=1)  # 32 hidden units, output size 1\n",
        "ltc_cell = LTCCell(wiring)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1, X_train.shape[2])),  # Input shape (time steps, features)\n",
        "    tf.keras.layers.RNN(ltc_cell, return_sequences=False),  # LTC layer\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test set (part of the 70%)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test MAE (70% data): {test_mae:.4f}')\n",
        "\n",
        "# Predict and evaluate on the remaining 30%\n",
        "y_pred = model.predict(X_predict)\n",
        "mae_so2 = np.mean(np.abs(y_predict - y_pred))\n",
        "print(f'MAE on 30% prediction data: {mae_so2:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z3TvVzY4cgq",
        "outputId": "126809b9-9bf5-4b9b-9afe-ca54a1ca1ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "263/263 [==============================] - 12s 11ms/step - loss: 6670.1426 - mae: 65.0659 - val_loss: 6822.4121 - val_mae: 65.6361\n",
            "Epoch 2/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 6179.9619 - mae: 61.1870 - val_loss: 6245.0312 - val_mae: 61.0795\n",
            "Epoch 3/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 5511.0762 - mae: 55.4295 - val_loss: 5417.5620 - val_mae: 53.9132\n",
            "Epoch 4/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 4673.8931 - mae: 47.5385 - val_loss: 4549.8149 - val_mae: 45.8877\n",
            "Epoch 5/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 3884.3210 - mae: 40.0351 - val_loss: 3769.5925 - val_mae: 39.0890\n",
            "Epoch 6/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3234.6785 - mae: 34.8132 - val_loss: 3171.2273 - val_mae: 34.9375\n",
            "Epoch 7/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2794.5435 - mae: 32.4128 - val_loss: 2801.9058 - val_mae: 33.4985\n",
            "Epoch 8/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 2560.0510 - mae: 32.2088 - val_loss: 2620.8765 - val_mae: 33.5731\n",
            "Epoch 9/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 2467.3032 - mae: 32.8657 - val_loss: 2550.5203 - val_mae: 34.1272\n",
            "Epoch 10/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 2435.4998 - mae: 33.4307 - val_loss: 2457.3191 - val_mae: 33.0675\n",
            "Epoch 11/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1851.0757 - mae: 22.2184 - val_loss: 1603.4548 - val_mae: 16.9872\n",
            "Epoch 12/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1333.9214 - mae: 13.8718 - val_loss: 1228.4476 - val_mae: 13.0638\n",
            "Epoch 13/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1043.0563 - mae: 10.9926 - val_loss: 963.6887 - val_mae: 10.3484\n",
            "Epoch 14/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 826.4097 - mae: 8.7088 - val_loss: 765.3123 - val_mae: 7.9387\n",
            "Epoch 15/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 662.2145 - mae: 6.8017 - val_loss: 616.5771 - val_mae: 6.3329\n",
            "Epoch 16/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 537.0643 - mae: 5.6995 - val_loss: 501.8946 - val_mae: 5.3787\n",
            "Epoch 17/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 439.2551 - mae: 4.9055 - val_loss: 415.2708 - val_mae: 4.7989\n",
            "Epoch 18/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 362.0369 - mae: 4.2997 - val_loss: 345.9234 - val_mae: 3.9887\n",
            "Epoch 19/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 300.2669 - mae: 3.7810 - val_loss: 292.1073 - val_mae: 3.5400\n",
            "Epoch 20/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 250.4044 - mae: 3.3704 - val_loss: 247.8644 - val_mae: 3.2783\n",
            "Epoch 21/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 210.2854 - mae: 3.0742 - val_loss: 210.9832 - val_mae: 3.0774\n",
            "Epoch 22/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 177.9952 - mae: 2.8467 - val_loss: 180.8369 - val_mae: 2.6644\n",
            "Epoch 23/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 151.4766 - mae: 2.6341 - val_loss: 155.4557 - val_mae: 2.4513\n",
            "Epoch 24/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 128.3020 - mae: 2.3798 - val_loss: 134.1664 - val_mae: 2.3012\n",
            "Epoch 25/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 109.2505 - mae: 2.2344 - val_loss: 115.3934 - val_mae: 2.1073\n",
            "Epoch 26/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 92.8845 - mae: 2.0292 - val_loss: 99.5636 - val_mae: 1.9919\n",
            "Epoch 27/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 79.4457 - mae: 1.9723 - val_loss: 85.5248 - val_mae: 1.8908\n",
            "Epoch 28/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 67.6104 - mae: 1.7775 - val_loss: 74.8378 - val_mae: 1.7495\n",
            "Epoch 29/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 57.9998 - mae: 1.6467 - val_loss: 64.5231 - val_mae: 1.5451\n",
            "Epoch 30/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 49.9393 - mae: 1.5393 - val_loss: 57.0248 - val_mae: 1.7126\n",
            "Epoch 31/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 43.0768 - mae: 1.4556 - val_loss: 49.6754 - val_mae: 1.3243\n",
            "Epoch 32/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 37.3253 - mae: 1.3388 - val_loss: 46.4030 - val_mae: 1.4699\n",
            "Epoch 33/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 32.4546 - mae: 1.2795 - val_loss: 39.1084 - val_mae: 1.1652\n",
            "Epoch 34/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 28.0706 - mae: 1.1546 - val_loss: 34.7420 - val_mae: 1.0685\n",
            "Epoch 35/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 24.2912 - mae: 0.9751 - val_loss: 31.1091 - val_mae: 0.9434\n",
            "Epoch 36/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 21.3354 - mae: 0.9269 - val_loss: 27.4222 - val_mae: 0.7859\n",
            "Epoch 37/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 18.8234 - mae: 0.8206 - val_loss: 24.9431 - val_mae: 0.8763\n",
            "Epoch 38/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 16.7481 - mae: 0.7631 - val_loss: 22.3799 - val_mae: 0.8031\n",
            "Epoch 39/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 15.0422 - mae: 0.7212 - val_loss: 20.1255 - val_mae: 0.7032\n",
            "Epoch 40/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 13.4127 - mae: 0.6687 - val_loss: 18.2097 - val_mae: 0.7564\n",
            "Epoch 41/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 12.0675 - mae: 0.6036 - val_loss: 16.2747 - val_mae: 0.7097\n",
            "Epoch 42/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 11.0445 - mae: 0.6742 - val_loss: 14.4957 - val_mae: 0.6300\n",
            "Epoch 43/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 9.8674 - mae: 0.5839 - val_loss: 12.9285 - val_mae: 0.6192\n",
            "Epoch 44/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 9.2798 - mae: 0.6685 - val_loss: 11.7085 - val_mae: 0.6969\n",
            "Epoch 45/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8.5104 - mae: 0.6477 - val_loss: 10.2781 - val_mae: 0.5174\n",
            "Epoch 46/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 7.6715 - mae: 0.5795 - val_loss: 9.5755 - val_mae: 0.6732\n",
            "Epoch 47/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 7.0488 - mae: 0.5398 - val_loss: 8.4894 - val_mae: 0.6162\n",
            "Epoch 48/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 6.6812 - mae: 0.6371 - val_loss: 7.3828 - val_mae: 0.5970\n",
            "Epoch 49/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 6.2416 - mae: 0.6445 - val_loss: 6.5011 - val_mae: 0.5435\n",
            "Epoch 50/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 5.8974 - mae: 0.6572 - val_loss: 6.9739 - val_mae: 0.9552\n",
            "83/83 [==============================] - 0s 4ms/step - loss: 20.8571 - mae: 0.9726\n",
            "Test MAE (70% data): 0.9726\n",
            "177/177 [==============================] - 1s 3ms/step\n",
            "MAE on 30% prediction data: 45.6571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from ncps.wirings import AutoNCP\n",
        "from ncps.tf import LTCCell\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('delhi_aqi.csv')  # Replace with the actual file name\n",
        "\n",
        "# Convert 'date' column to datetime and extract useful features\n",
        "df['Datetime'] = pd.to_datetime(df['date'])\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['date', 'Datetime'], axis=1, inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "features = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Hour', 'DayOfWeek', 'Day', 'Month']\n",
        "target = 'pm2_5'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into 70% and 30%\n",
        "split_index = int(0.7 * len(X))\n",
        "X_train_test, X_predict = X[:split_index], X[split_index:]\n",
        "y_train_test, y_predict = y[:split_index], y[split_index:]\n",
        "\n",
        "# Further split the 70% data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LNN input\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_predict = X_predict.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n",
        "\n",
        "# Build the LNN model\n",
        "wiring = AutoNCP(32, output_size=1)  # 32 hidden units, output size 1\n",
        "ltc_cell = LTCCell(wiring)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1, X_train.shape[2])),  # Input shape (time steps, features)\n",
        "    tf.keras.layers.RNN(ltc_cell, return_sequences=False),  # LTC layer\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test set (part of the 70%)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test MAE (70% data): {test_mae:.4f}')\n",
        "\n",
        "# Predict and evaluate on the remaining 30%\n",
        "y_pred = model.predict(X_predict)\n",
        "mae_pm2_5 = np.mean(np.abs(y_predict - y_pred))\n",
        "print(f'MAE on 30% prediction data: {mae_pm2_5:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmeZfl4j4xyF",
        "outputId": "778486aa-5063-4c2a-e7a7-15d160eb782f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "263/263 [==============================] - 13s 13ms/step - loss: 113524.9297 - mae: 245.3976 - val_loss: 122089.1641 - val_mae: 249.2507\n",
            "Epoch 2/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 112102.8516 - mae: 242.4680 - val_loss: 120259.8281 - val_mae: 245.5519\n",
            "Epoch 3/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 109628.1484 - mae: 237.2939 - val_loss: 117014.2578 - val_mae: 238.8531\n",
            "Epoch 4/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 105890.3438 - mae: 229.2852 - val_loss: 112353.4766 - val_mae: 228.9118\n",
            "Epoch 5/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 100569.1719 - mae: 217.6011 - val_loss: 106519.4688 - val_mae: 216.2469\n",
            "Epoch 6/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 94641.8125 - mae: 204.3873 - val_loss: 100175.1641 - val_mae: 202.7583\n",
            "Epoch 7/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 88286.5391 - mae: 191.0212 - val_loss: 93522.2891 - val_mae: 189.5681\n",
            "Epoch 8/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 81744.4531 - mae: 178.5411 - val_loss: 86795.2422 - val_mae: 177.6393\n",
            "Epoch 9/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 75365.7344 - mae: 168.0604 - val_loss: 80400.3750 - val_mae: 168.4986\n",
            "Epoch 10/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 69515.3984 - mae: 160.7132 - val_loss: 74689.7422 - val_mae: 162.8489\n",
            "Epoch 11/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 64486.2656 - mae: 156.2722 - val_loss: 69922.7500 - val_mae: 160.5811\n",
            "Epoch 12/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 60456.8125 - mae: 155.0392 - val_loss: 66233.8516 - val_mae: 160.9259\n",
            "Epoch 13/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 57487.3828 - mae: 156.0639 - val_loss: 63595.9375 - val_mae: 162.9302\n",
            "Epoch 14/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 55524.1484 - mae: 158.7860 - val_loss: 61907.7734 - val_mae: 165.7713\n",
            "Epoch 15/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 54369.8711 - mae: 161.4239 - val_loss: 60932.2188 - val_mae: 168.6421\n",
            "Epoch 16/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 53755.9023 - mae: 164.0520 - val_loss: 60409.8711 - val_mae: 171.2997\n",
            "Epoch 17/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 53475.2891 - mae: 166.2447 - val_loss: 60177.9453 - val_mae: 173.1830\n",
            "Epoch 18/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 49482.0352 - mae: 148.3639 - val_loss: 46100.9844 - val_mae: 103.9172\n",
            "Epoch 19/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 35671.6328 - mae: 86.0486 - val_loss: 38318.7227 - val_mae: 82.9369\n",
            "Epoch 20/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 29528.8281 - mae: 71.4359 - val_loss: 32448.7988 - val_mae: 71.0557\n",
            "Epoch 21/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 24632.0645 - mae: 58.9651 - val_loss: 27572.0605 - val_mae: 60.3525\n",
            "Epoch 22/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 20675.6348 - mae: 50.6381 - val_loss: 23559.3086 - val_mae: 53.9900\n",
            "Epoch 23/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 17389.8008 - mae: 44.4627 - val_loss: 20133.6035 - val_mae: 47.9475\n",
            "Epoch 24/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 14646.2979 - mae: 39.0184 - val_loss: 17191.5332 - val_mae: 42.0916\n",
            "Epoch 25/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 12346.5186 - mae: 34.3552 - val_loss: 14685.3887 - val_mae: 37.1878\n",
            "Epoch 26/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 10410.7246 - mae: 30.2689 - val_loss: 12573.4160 - val_mae: 33.3548\n",
            "Epoch 27/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8783.7490 - mae: 26.8926 - val_loss: 10762.6162 - val_mae: 29.7594\n",
            "Epoch 28/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 7398.6313 - mae: 23.7837 - val_loss: 9184.0371 - val_mae: 26.3886\n",
            "Epoch 29/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 6230.6943 - mae: 21.0292 - val_loss: 7841.1133 - val_mae: 24.3251\n",
            "Epoch 30/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 5244.1318 - mae: 18.4761 - val_loss: 6697.7939 - val_mae: 20.7249\n",
            "Epoch 31/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 4412.2861 - mae: 15.9954 - val_loss: 5711.9087 - val_mae: 18.8508\n",
            "Epoch 32/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 3718.1477 - mae: 14.0822 - val_loss: 4866.5317 - val_mae: 15.9252\n",
            "Epoch 33/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3129.3691 - mae: 12.1694 - val_loss: 4150.1753 - val_mae: 13.9948\n",
            "Epoch 34/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2637.7412 - mae: 10.6895 - val_loss: 3539.6860 - val_mae: 12.4085\n",
            "Epoch 35/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2220.7407 - mae: 9.2716 - val_loss: 3019.1960 - val_mae: 10.7233\n",
            "Epoch 36/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 1871.0006 - mae: 8.2803 - val_loss: 2581.6877 - val_mae: 10.7621\n",
            "Epoch 37/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1575.7782 - mae: 7.3179 - val_loss: 2192.4062 - val_mae: 8.5933\n",
            "Epoch 38/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1330.0682 - mae: 6.6260 - val_loss: 1880.9620 - val_mae: 8.0908\n",
            "Epoch 39/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1125.2936 - mae: 6.0373 - val_loss: 1609.5521 - val_mae: 7.2235\n",
            "Epoch 40/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 953.1349 - mae: 5.6957 - val_loss: 1379.0057 - val_mae: 6.4031\n",
            "Epoch 41/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 808.5059 - mae: 5.3582 - val_loss: 1183.7795 - val_mae: 6.4501\n",
            "Epoch 42/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 686.0913 - mae: 4.7785 - val_loss: 1015.6620 - val_mae: 6.0409\n",
            "Epoch 43/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 584.9332 - mae: 4.4781 - val_loss: 877.4635 - val_mae: 4.9611\n",
            "Epoch 44/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 500.4979 - mae: 4.2954 - val_loss: 772.6823 - val_mae: 6.3961\n",
            "Epoch 45/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 427.8216 - mae: 4.1172 - val_loss: 657.0406 - val_mae: 4.4514\n",
            "Epoch 46/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 364.9458 - mae: 3.8333 - val_loss: 573.6905 - val_mae: 4.1624\n",
            "Epoch 47/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 314.6621 - mae: 3.8609 - val_loss: 498.9249 - val_mae: 3.9435\n",
            "Epoch 48/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 268.2318 - mae: 3.4403 - val_loss: 440.1939 - val_mae: 4.1253\n",
            "Epoch 49/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 230.8596 - mae: 3.3306 - val_loss: 388.3147 - val_mae: 3.8160\n",
            "Epoch 50/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 199.7426 - mae: 3.2557 - val_loss: 349.4758 - val_mae: 5.2081\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 161.1197 - mae: 4.3707\n",
            "Test MAE (70% data): 4.3707\n",
            "177/177 [==============================] - 1s 3ms/step\n",
            "MAE on 30% prediction data: 207.3983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from ncps.wirings import AutoNCP\n",
        "from ncps.tf import LTCCell\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('delhi_aqi.csv')  # Replace with the actual file name\n",
        "\n",
        "# Convert 'date' column to datetime and extract useful features\n",
        "df['Datetime'] = pd.to_datetime(df['date'])\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['date', 'Datetime'], axis=1, inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "features = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Hour', 'DayOfWeek', 'Day', 'Month']\n",
        "target = 'pm10'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into 70% and 30%\n",
        "split_index = int(0.7 * len(X))\n",
        "X_train_test, X_predict = X[:split_index], X[split_index:]\n",
        "y_train_test, y_predict = y[:split_index], y[split_index:]\n",
        "\n",
        "# Further split the 70% data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LNN input\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_predict = X_predict.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n",
        "\n",
        "# Build the LNN model\n",
        "wiring = AutoNCP(32, output_size=1)  # 32 hidden units, output size 1\n",
        "ltc_cell = LTCCell(wiring)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1, X_train.shape[2])),  # Input shape (time steps, features)\n",
        "    tf.keras.layers.RNN(ltc_cell, return_sequences=False),  # LTC layer\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test set (part of the 70%)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test MAE (70% data): {test_mae:.4f}')\n",
        "\n",
        "# Predict and evaluate on the remaining 30%\n",
        "y_pred = model.predict(X_predict)\n",
        "mae_pm10 = np.mean(np.abs(y_predict - y_pred))\n",
        "print(f'MAE on 30% prediction data: {mae_pm10:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBwyBxO544so",
        "outputId": "7923d890-cd88-4420-d5a4-5d29e246611b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "263/263 [==============================] - 12s 12ms/step - loss: 168009.1406 - mae: 308.5298 - val_loss: 181375.7031 - val_mae: 314.0916\n",
            "Epoch 2/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 166296.9219 - mae: 305.7374 - val_loss: 179184.2812 - val_mae: 310.5831\n",
            "Epoch 3/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 163379.1406 - mae: 300.9394 - val_loss: 175378.0781 - val_mae: 304.3942\n",
            "Epoch 4/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 158934.3125 - mae: 293.5067 - val_loss: 169864.6094 - val_mae: 295.1989\n",
            "Epoch 5/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 152628.7656 - mae: 282.5595 - val_loss: 162758.6094 - val_mae: 282.9694\n",
            "Epoch 6/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 144951.5781 - mae: 268.7771 - val_loss: 153726.5469 - val_mae: 267.1524\n",
            "Epoch 7/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 135463.3594 - mae: 251.6077 - val_loss: 143784.6875 - val_mae: 249.7512\n",
            "Epoch 8/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 125712.4922 - mae: 234.5071 - val_loss: 133705.5000 - val_mae: 232.9592\n",
            "Epoch 9/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 115975.1641 - mae: 218.3873 - val_loss: 123797.3516 - val_mae: 217.7637\n",
            "Epoch 10/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 106646.0625 - mae: 204.4687 - val_loss: 114474.7188 - val_mae: 205.4935\n",
            "Epoch 11/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 98117.8672 - mae: 193.5541 - val_loss: 106097.6328 - val_mae: 197.0120\n",
            "Epoch 12/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 90720.9062 - mae: 186.5470 - val_loss: 99041.3125 - val_mae: 192.2001\n",
            "Epoch 13/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 84669.7422 - mae: 183.0633 - val_loss: 93390.1719 - val_mae: 190.5496\n",
            "Epoch 14/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 80079.9609 - mae: 183.0050 - val_loss: 89202.6094 - val_mae: 191.5096\n",
            "Epoch 15/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 76903.8750 - mae: 184.7342 - val_loss: 86382.1953 - val_mae: 194.1597\n",
            "Epoch 16/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 74910.1641 - mae: 187.6310 - val_loss: 84683.3750 - val_mae: 197.1943\n",
            "Epoch 17/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 73782.2500 - mae: 190.3513 - val_loss: 83686.0391 - val_mae: 200.0066\n",
            "Epoch 18/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 57257.3672 - mae: 121.6130 - val_loss: 60634.2266 - val_mae: 109.7842\n",
            "Epoch 19/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 47281.6719 - mae: 96.2093 - val_loss: 52371.3750 - val_mae: 97.9286\n",
            "Epoch 20/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 40309.7930 - mae: 85.5051 - val_loss: 45282.8086 - val_mae: 87.5266\n",
            "Epoch 21/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 34432.9414 - mae: 76.0541 - val_loss: 39221.1523 - val_mae: 78.9870\n",
            "Epoch 22/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 29365.8242 - mae: 67.7058 - val_loss: 33851.2812 - val_mae: 69.4044\n",
            "Epoch 23/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 24944.9551 - mae: 58.8746 - val_loss: 29173.0527 - val_mae: 60.9387\n",
            "Epoch 24/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 21157.7812 - mae: 51.3861 - val_loss: 25096.1055 - val_mae: 53.7801\n",
            "Epoch 25/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 17903.6680 - mae: 44.0383 - val_loss: 21577.1836 - val_mae: 48.9281\n",
            "Epoch 26/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 15131.6670 - mae: 37.7689 - val_loss: 18441.2598 - val_mae: 40.5536\n",
            "Epoch 27/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 12788.0664 - mae: 32.8639 - val_loss: 15812.9521 - val_mae: 36.0659\n",
            "Epoch 28/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 10805.8838 - mae: 28.7789 - val_loss: 13565.0527 - val_mae: 32.5790\n",
            "Epoch 29/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 9128.9893 - mae: 25.6203 - val_loss: 11629.9697 - val_mae: 28.4813\n",
            "Epoch 30/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 7712.4365 - mae: 23.1150 - val_loss: 9944.7715 - val_mae: 25.9211\n",
            "Epoch 31/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 6507.4897 - mae: 20.1919 - val_loss: 8519.8320 - val_mae: 23.0533\n",
            "Epoch 32/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 5497.3687 - mae: 17.8156 - val_loss: 7289.8750 - val_mae: 20.9457\n",
            "Epoch 33/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 4646.2393 - mae: 15.8813 - val_loss: 6231.3428 - val_mae: 19.4466\n",
            "Epoch 34/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3926.2419 - mae: 14.1887 - val_loss: 5302.5830 - val_mae: 16.5962\n",
            "Epoch 35/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3317.2954 - mae: 12.6878 - val_loss: 4513.9824 - val_mae: 14.7497\n",
            "Epoch 36/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2805.4128 - mae: 11.3757 - val_loss: 3855.7224 - val_mae: 13.1583\n",
            "Epoch 37/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2374.9585 - mae: 10.1986 - val_loss: 3289.6399 - val_mae: 11.6244\n",
            "Epoch 38/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 2010.1222 - mae: 8.9774 - val_loss: 2803.2449 - val_mae: 10.5560\n",
            "Epoch 39/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1705.3217 - mae: 8.0983 - val_loss: 2392.7295 - val_mae: 9.5950\n",
            "Epoch 40/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1449.7732 - mae: 7.3588 - val_loss: 2045.6719 - val_mae: 7.9609\n",
            "Epoch 41/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1231.9531 - mae: 6.3265 - val_loss: 1755.4009 - val_mae: 8.0933\n",
            "Epoch 42/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1050.3634 - mae: 5.8145 - val_loss: 1508.7328 - val_mae: 6.6184\n",
            "Epoch 43/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 896.4373 - mae: 5.3934 - val_loss: 1309.2593 - val_mae: 9.0035\n",
            "Epoch 44/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 766.7614 - mae: 5.0843 - val_loss: 1118.7661 - val_mae: 6.4288\n",
            "Epoch 45/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 655.5650 - mae: 4.8795 - val_loss: 966.6548 - val_mae: 6.5066\n",
            "Epoch 46/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 559.4792 - mae: 4.4168 - val_loss: 824.5300 - val_mae: 4.8463\n",
            "Epoch 47/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 479.3824 - mae: 4.3667 - val_loss: 705.0590 - val_mae: 4.6475\n",
            "Epoch 48/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 409.7471 - mae: 4.0099 - val_loss: 607.3189 - val_mae: 3.8593\n",
            "Epoch 49/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 350.6249 - mae: 3.6938 - val_loss: 556.4142 - val_mae: 7.1598\n",
            "Epoch 50/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 301.5667 - mae: 3.4669 - val_loss: 469.5882 - val_mae: 5.6357\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 226.1843 - mae: 4.7204\n",
            "Test MAE (70% data): 4.7204\n",
            "177/177 [==============================] - 2s 3ms/step\n",
            "MAE on 30% prediction data: 251.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from ncps.wirings import AutoNCP\n",
        "from ncps.tf import LTCCell\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('delhi_aqi.csv')  # Replace with the actual file name\n",
        "\n",
        "# Convert 'date' column to datetime and extract useful features\n",
        "df['Datetime'] = pd.to_datetime(df['date'])\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['date', 'Datetime'], axis=1, inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "features = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Hour', 'DayOfWeek', 'Day', 'Month']\n",
        "target = 'nh3'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into 70% and 30%\n",
        "split_index = int(0.7 * len(X))\n",
        "X_train_test, X_predict = X[:split_index], X[split_index:]\n",
        "y_train_test, y_predict = y[:split_index], y[split_index:]\n",
        "\n",
        "# Further split the 70% data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LNN input\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_predict = X_predict.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n",
        "\n",
        "# Build the LNN model\n",
        "wiring = AutoNCP(32, output_size=1)  # 32 hidden units, output size 1\n",
        "ltc_cell = LTCCell(wiring)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1, X_train.shape[2])),  # Input shape (time steps, features)\n",
        "    tf.keras.layers.RNN(ltc_cell, return_sequences=False),  # LTC layer\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test set (part of the 70%)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test MAE (70% data): {test_mae:.4f}')\n",
        "\n",
        "# Predict and evaluate on the remaining 30%\n",
        "y_pred = model.predict(X_predict)\n",
        "mae_nh3 = np.mean(np.abs(y_predict - y_pred))\n",
        "print(f'MAE on 30% prediction data: {mae_nh3:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "corQsPUx5nTq",
        "outputId": "c67e5a21-bc9e-4a55-864d-6f1af23addb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "263/263 [==============================] - 12s 11ms/step - loss: 1281.1456 - mae: 24.9784 - val_loss: 1299.6122 - val_mae: 25.0355\n",
            "Epoch 2/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1189.2351 - mae: 23.0787 - val_loss: 1161.3015 - val_mae: 22.1479\n",
            "Epoch 3/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 1020.3853 - mae: 19.3773 - val_loss: 965.5653 - val_mae: 18.2215\n",
            "Epoch 4/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 844.9330 - mae: 16.2835 - val_loss: 803.9718 - val_mae: 16.0999\n",
            "Epoch 5/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 724.5480 - mae: 15.1474 - val_loss: 713.1530 - val_mae: 15.6950\n",
            "Epoch 6/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 672.4422 - mae: 15.3257 - val_loss: 681.9584 - val_mae: 16.0917\n",
            "Epoch 7/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 659.1028 - mae: 15.7351 - val_loss: 675.0172 - val_mae: 16.4434\n",
            "Epoch 8/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 657.3059 - mae: 16.0447 - val_loss: 673.9517 - val_mae: 16.5591\n",
            "Epoch 9/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 590.8469 - mae: 13.9758 - val_loss: 544.3541 - val_mae: 13.2954\n",
            "Epoch 10/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 491.2697 - mae: 12.1675 - val_loss: 457.7931 - val_mae: 12.1606\n",
            "Epoch 11/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 418.1883 - mae: 10.9721 - val_loss: 383.2846 - val_mae: 10.7751\n",
            "Epoch 12/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 357.1367 - mae: 9.7427 - val_loss: 324.3658 - val_mae: 9.6397\n",
            "Epoch 13/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 297.1773 - mae: 8.4565 - val_loss: 253.2712 - val_mae: 7.9539\n",
            "Epoch 14/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 233.7826 - mae: 6.6836 - val_loss: 191.0730 - val_mae: 5.9913\n",
            "Epoch 15/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 185.2856 - mae: 5.1812 - val_loss: 148.1627 - val_mae: 4.6221\n",
            "Epoch 16/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 150.5238 - mae: 4.1519 - val_loss: 117.4519 - val_mae: 3.6612\n",
            "Epoch 17/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 123.7954 - mae: 3.4139 - val_loss: 94.6524 - val_mae: 3.0577\n",
            "Epoch 18/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 102.5720 - mae: 2.9179 - val_loss: 77.1970 - val_mae: 2.7486\n",
            "Epoch 19/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 85.4861 - mae: 2.5701 - val_loss: 64.2457 - val_mae: 2.6775\n",
            "Epoch 20/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 71.0621 - mae: 2.2083 - val_loss: 51.0494 - val_mae: 1.9194\n",
            "Epoch 21/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 59.7642 - mae: 1.9908 - val_loss: 42.9562 - val_mae: 1.9602\n",
            "Epoch 22/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 49.8974 - mae: 1.7055 - val_loss: 34.7972 - val_mae: 1.5593\n",
            "Epoch 23/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 41.5287 - mae: 1.4632 - val_loss: 27.7925 - val_mae: 1.2377\n",
            "Epoch 24/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 34.6487 - mae: 1.3188 - val_loss: 23.4276 - val_mae: 1.3415\n",
            "Epoch 25/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 28.9246 - mae: 1.2027 - val_loss: 18.6216 - val_mae: 1.1394\n",
            "Epoch 26/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 23.9239 - mae: 1.0099 - val_loss: 14.4257 - val_mae: 0.8234\n",
            "Epoch 27/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 20.1680 - mae: 0.9809 - val_loss: 11.6971 - val_mae: 0.8835\n",
            "Epoch 28/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 16.7099 - mae: 0.8533 - val_loss: 9.3832 - val_mae: 0.6978\n",
            "Epoch 29/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 14.2906 - mae: 0.8862 - val_loss: 8.1675 - val_mae: 0.8259\n",
            "Epoch 30/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 11.9735 - mae: 0.7971 - val_loss: 6.4212 - val_mae: 0.6458\n",
            "Epoch 31/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 10.0686 - mae: 0.7311 - val_loss: 5.3581 - val_mae: 0.5978\n",
            "Epoch 32/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 8.7504 - mae: 0.7691 - val_loss: 4.4244 - val_mae: 0.5676\n",
            "Epoch 33/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 7.4845 - mae: 0.7245 - val_loss: 4.5266 - val_mae: 0.7727\n",
            "Epoch 34/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 6.3164 - mae: 0.6392 - val_loss: 3.6034 - val_mae: 0.6637\n",
            "Epoch 35/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 5.4032 - mae: 0.6201 - val_loss: 2.7239 - val_mae: 0.5089\n",
            "Epoch 36/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4.6619 - mae: 0.5941 - val_loss: 2.5896 - val_mae: 0.6123\n",
            "Epoch 37/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 4.1272 - mae: 0.6226 - val_loss: 2.0333 - val_mae: 0.5517\n",
            "Epoch 38/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 3.7019 - mae: 0.6597 - val_loss: 2.0759 - val_mae: 0.7876\n",
            "Epoch 39/50\n",
            "263/263 [==============================] - 3s 12ms/step - loss: 3.2333 - mae: 0.6309 - val_loss: 1.3985 - val_mae: 0.5449\n",
            "Epoch 40/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2.7152 - mae: 0.5536 - val_loss: 2.1284 - val_mae: 0.8204\n",
            "Epoch 41/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 2.4590 - mae: 0.5858 - val_loss: 1.0431 - val_mae: 0.4116\n",
            "Epoch 42/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1.9954 - mae: 0.4713 - val_loss: 1.0845 - val_mae: 0.5302\n",
            "Epoch 43/50\n",
            "263/263 [==============================] - 2s 9ms/step - loss: 1.9840 - mae: 0.5805 - val_loss: 0.8944 - val_mae: 0.4236\n",
            "Epoch 44/50\n",
            "263/263 [==============================] - 3s 11ms/step - loss: 1.8086 - mae: 0.5629 - val_loss: 0.5722 - val_mae: 0.2902\n",
            "Epoch 45/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1.6831 - mae: 0.5813 - val_loss: 0.8576 - val_mae: 0.6333\n",
            "Epoch 46/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1.5036 - mae: 0.5719 - val_loss: 1.0141 - val_mae: 0.7981\n",
            "Epoch 47/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1.2971 - mae: 0.5180 - val_loss: 0.4257 - val_mae: 0.3483\n",
            "Epoch 48/50\n",
            "263/263 [==============================] - 2s 8ms/step - loss: 1.1617 - mae: 0.4931 - val_loss: 0.7151 - val_mae: 0.5363\n",
            "Epoch 49/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 1.0782 - mae: 0.5033 - val_loss: 0.7420 - val_mae: 0.4480\n",
            "Epoch 50/50\n",
            "263/263 [==============================] - 3s 10ms/step - loss: 0.8561 - mae: 0.4124 - val_loss: 0.3257 - val_mae: 0.3498\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.3673 - mae: 0.3468\n",
            "Test MAE (70% data): 0.3468\n",
            "177/177 [==============================] - 1s 3ms/step\n",
            "MAE on 30% prediction data: 22.6086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_mae = ( mae_co + mae_nh3 + mae_pm10 + mae_pm2_5 + mae_so2 + mae_o3 + mae_no + mae_no2 ) / 8\n",
        "avg_mae"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAZfVFSz5uIT",
        "outputId": "6db4929f-55d5-4820-bf09-a13d1158477f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.2087875"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}